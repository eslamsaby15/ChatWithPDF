{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f15d59",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A simple **Chat-with-Multi PDFs / Docs** app for **Retrieval-Augmented Generation (RAG)**.  \n",
    "It allows you to ask questions about the contents of PDFs and DOC files, and the app will provide relevant responses.\n",
    "\n",
    "![Chatbot Diagram](../src/images/graphWorkFlow.jpg)\n",
    "\n",
    "This app is built using [**LangGraph**](https://www.langchain.com/langgraph) and [**LangChain**](https://www.langchain.com/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0fefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Setting\n",
    "import os \n",
    "import dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ['GOOGLE_API_KEY'] = dotenv.get_key(key_to_get='GOOGLE_API_KEY', dotenv_path='.env')\n",
    "llm = init_chat_model(model = 'gemini-2.5-flash' , model_provider='google-genai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3005b",
   "metadata": {},
   "source": [
    "# 1. Preprocess documents\n",
    "\n",
    "- In this step we read files to get the full text and split them into small chunks and embed them to store in Vectore DataBase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5748e75",
   "metadata": {},
   "source": [
    "##### 1) Reading Files using PyPDF2 to extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def get_content_pages(files:  list[str]) -> str:\n",
    "    \"\"\"Read PDF or TXT files and return their combined text\"\"\"\n",
    "    ftext = \"\"\n",
    "    for file_path in files:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            pdf_loader = PdfReader(file_path)\n",
    "            for page in pdf_loader.pages:\n",
    "                ftext += page.extract_text() or \"\"  \n",
    "        else:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                ftext += f.read()\n",
    "        ftext += \"\\n\"\n",
    "    return ftext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d16b1",
   "metadata": {},
   "source": [
    "##### 2) Split Full text into small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f1aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_chunks(text : str , chunk_size :int = 600, chunk_overlap : int = 50) :\n",
    "    \"\"\"Split to small and overla chunks\"\"\"\n",
    "    splitter= RecursiveCharacterTextSplitter(\"\\n\", chunk_size =chunk_size , \n",
    "                                         chunk_overlap =chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af101dd1",
   "metadata": {},
   "source": [
    "##### 3) Initialize Vector Database\n",
    "\n",
    "1. Generate embeddings from chunks using the **[MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)** model from SentenceTransformers.\n",
    "\n",
    "2. Store the embeddings in a vector database using **Chroma** from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9326b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "def create_vectorDB(chunks : list[str] ,\n",
    "                    embedding_name  : str = \"sentence-transformers/all-MiniLM-L6-v2\" ) : \n",
    "    \"\"\"Generate and store embedding of chunks into vector database\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name = embedding_name)\n",
    "    vectordb = InMemoryVectorStore.from_texts(chunks , embedding_model)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a14f2f",
   "metadata": {},
   "source": [
    "##### 4) concate components together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbd992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessDocuments(files:list[str]) :\n",
    "    \"\"\"concat all together\"\"\" \n",
    "    \n",
    "    content = get_content_pages(files=files)\n",
    "    chunks = split_chunks(content)\n",
    "    vectorDB = create_vectorDB(chunks)\n",
    "    return vectorDB , chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3e8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szeya\\AppData\\Local\\Temp\\ipykernel_25508\\2406254441.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name = embedding_name)\n",
      "c:\\Users\\szeya\\miniconda3\\envs\\chat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# testing functions\n",
    "pdfs = [os.path.join('data' , x) for x in os.listdir(\"data\")]  #pdf paths\n",
    "vector_db ,chunks = ProcessDocuments(pdfs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285865f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba43e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- *1* ---------------------------------------------\n",
      "E1+\n",
      "E2+\n",
      "E3+\n",
      "E4+\n",
      "E5+……………\n",
      "U\n",
      "U\n",
      "U\n",
      "U…logitslogitslogitslogitslogits\n",
      "Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\n",
      "get encoded, passed through a set of stacked transformer blocks, and then a language model\n",
      "head that predicts the next token.\n",
      "Fig. 9.1 sketches the transformer architecture. A transformer has three major\n",
      "components. At the center are columns of transformer blocks . Each block is a\n",
      "multilayer network (a multi-head attention layer, feedforward networks and layer\n",
      "--------------------------------------------- *2* ---------------------------------------------\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "--------------------------------------------- *3* ---------------------------------------------\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n"
     ]
    }
   ],
   "source": [
    "query = \"transformer encoder\" \n",
    "similar_docs = vector_db.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(\"---\" * 15, f\"*{i+1}*\", \"---\" * 15)\n",
    "\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32a810",
   "metadata": {},
   "source": [
    "# 2. Generate a Retriever Tool using LangChain\n",
    "\n",
    "- Use `create_retriever_tool` from **LangChain** to generate a retriever tool from the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95436b8",
   "metadata": {},
   "source": [
    "##### 1) retriever_tool Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a7d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1+\n",
      "E2+\n",
      "E3+\n",
      "E4+\n",
      "E5+……………\n",
      "U\n",
      "U\n",
      "U\n",
      "U…logitslogitslogitslogitslogits\n",
      "Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\n",
      "get encoded, passed through a set of stacked transformer blocks, and then a language model\n",
      "head that predicts the next token.\n",
      "Fig. 9.1 sketches the transformer architecture. A transformer has three major\n",
      "components. At the center are columns of transformer blocks . Each block is a\n",
      "multilayer network (a multi-head attention layer, feedforward networks and layer\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "sampled so that their combined length was less than the 512 token input. Tokens\n",
      "within these sentence pairs were then masked using the MLM approach with the\n",
      "combined loss from the MLM and NSP objectives used for a ﬁnal loss. Because this\n",
      "ﬁnal loss is backpropagated through the entire transformer, the embeddings at each\n",
      "transformer layer will learn representations that are useful for predicting words from\n",
      "their neighbors. Since the [CLS] tokens are the direct input to the NSP classiﬁer,\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool \n",
    "\n",
    "retriver = vector_db.as_retriever()\n",
    "retrivertool = create_retriever_tool(retriver , \n",
    "                                     \"retriever_tool\" ,\n",
    "                                     \"Search and return information about input context\" )\n",
    "\n",
    "print(retrivertool.invoke({\"query\": query}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b881946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RetriverAgent = create_react_agent(\n",
    "            llm,\n",
    "            tools=[retrivertool],\n",
    "            name=\"RetriverAgent\",\n",
    "            prompt=(\n",
    "                \"You are a retriever agent.\\n\"\n",
    "                \"- Always call the retriever_tool with the user query.\\n\"\n",
    "                \"- Return ONLY the page_content of the retrieved documents.\\n\"\n",
    "                \"- Do not summarize or rephrase.\\n\"\n",
    "                \"- Don't return repeated retrieved chunks.\\n\"\n",
    "                \"- If you didn't find similar text return 'I can't find it'\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b8abf",
   "metadata": {},
   "source": [
    "# 3. Detect Language and Dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5a5e3",
   "metadata": {},
   "source": [
    "##### 1) Detect Language using Prompt Template with structure ouput and Dialect if the query language is ar\n",
    "\n",
    "#####  Detect Arabic dialect\n",
    "\n",
    "- Using [IbrahimAmin/marbertv2-arabic-written-dialect-classifier](https://huggingface.co/IbrahimAmin/marbertv2-arabic-written-dialect-classifier)  \n",
    "- The model predicts one of **5 Arabic dialects**:\n",
    "\n",
    "| Code | Dialect | Region / Notes |\n",
    "|------|---------|----------------|\n",
    "| MAGHREB | Maghreb dialect | Northwest Africa (Morocco, Algeria, Tunisia, Libya, Mauritania) |\n",
    "| LEV     | Levantine dialect | Lebanon, Syria, Jordan, Palestine |\n",
    "| MSA     | Modern Standard Arabic | Formal Arabic (books, news, official use) |\n",
    "| GLF     | Gulf dialect | Saudi Arabia, UAE, Kuwait, Bahrain, Qatar, Oman |\n",
    "| EGY     | Egyptian dialect | Egypt |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21fafd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "class LanguageDetector(BaseModel):\n",
    "    language: str = Field(\n",
    "        description=\"Detected language of the question, represented in a two-character ISO 639-1 code.\"\n",
    "    )\n",
    "\n",
    "\n",
    "dialect_model_name = \"IbrahimAmin/marbertv2-arabic-written-dialect-classifier\"\n",
    "\n",
    "dialect_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(dialect_model_name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(dialect_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def detecting_language(state: MessagesState):\n",
    "    \n",
    "    question = state[\"messages\"][0].content\n",
    "    detectionmodel = llm.with_structured_output(LanguageDetector)\n",
    "\n",
    "    LANGUAGE_DETECTOR_TEMPLATE = \"\\n\\n\".join([\n",
    "        \"You are a language detector assessing to return the language of the question from a user.\",\n",
    "        \"Here is the user question: {question}\",\n",
    "        \"# Instructions:\",\n",
    "        \"- Return only the two-character ISO 639-1 code for the language.\",\n",
    "        \"- Base detection on the language of the question itself (its structure and wording), not on individual foreign words inside it.\",\n",
    "        \"- Focus especially on the interrogative word (e.g., what, how, من, ماذا) and the main verb or auxiliary verb.\"\n",
    "    ])\n",
    "\n",
    "\n",
    "    detection_prompt = PromptTemplate(\n",
    "        template=LANGUAGE_DETECTOR_TEMPLATE,\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "\n",
    "    prompt = detection_prompt.format(question=question)\n",
    "    response: LanguageDetector = detectionmodel.invoke(prompt)\n",
    "\n",
    "    # dialect\n",
    "    dialect = None\n",
    "    if response.language == \"ar\":\n",
    "        preds = dialect_pipeline(question, top_k=None)\n",
    "        if preds:\n",
    "            best = max(preds, key=lambda x: x[\"score\"])\n",
    "            dialect = best[\"label\"]\n",
    "\n",
    "    return {\"detected_lang\": response.language, \"dialect\": dialect}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1e292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic Question: {'detected_lang': 'ar', 'dialect': 'MSA'}\n",
      "--------------------------------------------------\n",
      "English Question: {'detected_lang': 'en', 'dialect': None}\n",
      "--------------------------------------------------\n",
      "French Question: {'detected_lang': 'fr', 'dialect': None}\n",
      "--------------------------------------------------\n",
      "Spanish Question: {'detected_lang': 'es', 'dialect': None}\n",
      "--------------------------------------------------\n",
      "German Question: {'detected_lang': 'de', 'dialect': None}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "\n",
    "examples = [\n",
    "    (\"Arabic Question\", MessagesState(messages=[HumanMessage(content=\"ما هي أنواع Transformer؟\")])),\n",
    "    (\"English Question\", MessagesState(messages=[HumanMessage(content=\"What are the types of Transformer?\")])),\n",
    "    (\"French Question\", MessagesState(messages=[HumanMessage(content=\"Qu'est-ce qu'un transformateur?\")])),\n",
    "    (\"Spanish Question\", MessagesState(messages=[HumanMessage(content=\"¿Cuáles son los tipos de transformadores?\")])),\n",
    "    (\"German Question\", MessagesState(messages=[HumanMessage(content=\"Welche Arten von Transformatoren gibt es?\")])) ,\n",
    "]\n",
    "\n",
    "\n",
    "for name, state in examples:\n",
    "    result = detecting_language(state)\n",
    "    print(f\"{name}: {result}\")\n",
    "    print(\"-\" * 50)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accf8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGY Example: {'detected_lang': 'ar', 'dialect': 'EGY'}\n",
      "--------------------------------------------------\n",
      "LEV Example 2: {'detected_lang': 'ar', 'dialect': 'LEV'}\n",
      "--------------------------------------------------\n",
      "GLF Example: {'detected_lang': 'ar', 'dialect': 'GLF'}\n",
      "--------------------------------------------------\n",
      "LEV Example: {'detected_lang': 'ar', 'dialect': 'LEV'}\n",
      "--------------------------------------------------\n",
      "MSA Example: {'detected_lang': 'ar', 'dialect': 'MSA'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "import time\n",
    "examples = [\n",
    "    (\"EGY Example\", MessagesState(messages=[HumanMessage(content=\"ازيك يصحبي قولي انواع l\")])),\n",
    "    (\"LEV Example 2\", MessagesState(messages=[HumanMessage(content=\"عامل اي يازلمي اليوم ممكن تقولي types of ML\")])),\n",
    "    (\"GLF Example\", MessagesState(messages=[HumanMessage(content=\"شلونك يا طويل العمر؟ ممكن تقول لي examples of ML\")])),\n",
    "    (\"LEV Example\", MessagesState(messages=[HumanMessage(content=\"كيفك يا زلمي؟ شو الأخبار؟ give me types of ML\")])),\n",
    "    (\"MSA Example\", MessagesState(messages=[HumanMessage(content=\"ما هي أنواع   ML techniques؟\")]))\n",
    "]\n",
    "\n",
    "\n",
    "for name, state in examples:\n",
    "    result = detecting_language(state)\n",
    "    print(f\"{name}: {result}\")\n",
    "    print(\"-\" * 50)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f172ea",
   "metadata": {},
   "source": [
    "##### 2) Translate Query to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d73a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "def TranslateQuery(state :MessagesState):\n",
    "    \"Machine Translation to translate Queries to english text\"\n",
    "    question = state['messages'][0].content\n",
    "\n",
    "    msg_prompt = f\"\"\"\n",
    "                    You are a Machine Translation (MT) system.\n",
    "                    Your task: translate the user question to English text.\n",
    "\n",
    "                    Instructions:\n",
    "                    1. Translate the question to English as accurately as possible.\n",
    "                    2. Do not add explanations, comments, or extra content.\n",
    "                    3. Do not attempt to clarify or modify the meaning.\n",
    "                    4. Keep the original meaning exactly.\n",
    "\n",
    "                    User question: \"{question}\"\n",
    "                    \"\"\"\n",
    "\n",
    "    prompt_msg_template = PromptTemplate(\n",
    "        template=msg_prompt,\n",
    "        input_variables=['question']\n",
    "    )\n",
    "\n",
    "    resonong = llm.invoke([{'role': 'user', 'content': prompt_msg_template.format(question=question)}])\n",
    "\n",
    "    return {'messages': [HumanMessage(content=resonong.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd5fd268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you differentiate between types of transformers?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input_state = {\n",
    "    \"messages\": convert_to_messages([\n",
    "        {\"role\": \"user\", \"content\": \"تقدر تفرق بين انواع transformers \"}\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "response = TranslateQuery(input_state)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be0803",
   "metadata": {},
   "source": [
    "# 4. Grader\n",
    "- Grander is a score computed from llm to determine whether the retrieved documents are relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e612d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "from typing import Literal\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "class GraderDocument(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "    binary_score : str = Field(description= \"Relevance score: 'yes' if relevant, or 'no' if not relevant\")\n",
    "\n",
    "\n",
    "def GraderDocumentAgent(state:MessagesState)-> Literal['Chit-ChatAgent' ,'AnswerAgent'] :\n",
    "    question = state['messages'][0].content\n",
    "    context = state['messages'][-1].content\n",
    "\n",
    "      \n",
    "    gradermodel = llm.with_structured_output(GraderDocument)\n",
    "    \n",
    "    GRADE_PROMPT = '\\n\\n'.join([\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\",\n",
    "    \"Here is the retrieved document: \\n\\n {context}\",\n",
    "    \"Here is the user question: {question}\",\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant\",\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"])\n",
    "    \n",
    "    prompt = PromptTemplate(template= GRADE_PROMPT , input_variables=['question', 'context'])\n",
    "    \n",
    "    prompt_template = prompt.format(question = question , context = context)\n",
    "    response =  gradermodel.invoke(\n",
    "        [HumanMessage(content=prompt_template) ]\n",
    "    )\n",
    "\n",
    "    score = response.binary_score\n",
    "    \n",
    "    if score == 'yes' :\n",
    "        return \"AnswerAgent\"\n",
    "    else : \n",
    "        return 'Chit-ChatAgent'\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f82013ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chit-ChatAgent'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does machine learning?\",\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retriever_tool\",\n",
    "                        \"args\": {\"query\": \"Supervised learning, Unsupervised learning, Reinforcement learning.\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "GraderDocumentAgent(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69180bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnswerAgent'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does machine learning?\",\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retriever_tool\",\n",
    "                        \"args\": {\"query\": \"Supervised learning, Unsupervised learning, Reinforcement learning.\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \n",
    "             \"content\": \"\"\"Machine learning is a field of artificial intelligence that focuses on building models that can learn from data and make predictions or decisions without being explicitly programmed. \n",
    "                            It is commonly used for tasks likelassification, regression, and pattern recognition. \"\"\", \n",
    "            \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "GraderDocumentAgent(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098f4f2",
   "metadata": {},
   "source": [
    "# 5. Generate an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6acf208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateAnswer(state:MessagesState) :\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "\n",
    "    GENERATE_PROMPT =\"\\n\".join([\n",
    "    \"You are an assistant for question-answering tasks. \",\n",
    "    \"Use the following pieces of retrieved context to answer the question. \",\n",
    "    \"Answer as many questions as possible and make it a simple temporary one\"\n",
    "    \"- Generate english text only\"\n",
    "    \"Question: {question} \\n\",\n",
    "    \"Context: {context}\" ]\n",
    "    )\n",
    "\n",
    "    question = state.get(\"translated_query\", state[\"messages\"][0].content)\n",
    "    print(\n",
    "      \"********************************\" , question , \"+++++++++++++++++++++++++\"\n",
    "    )\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm.invoke([{\"role\": \"user\", \n",
    "                            \"content\": prompt}])\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9ae8b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************** What are the main types of machine learning? +++++++++++++++++++++++++\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The main types of machine learning are:\n",
      "*   Supervised learning\n",
      "*   Unsupervised learning\n",
      "*   Reinforcement learning\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the main types of machine learning?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_tool\",\n",
    "                        \"args\": {\"query\": \"types of machine learning\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Machine learning is commonly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = GenerateAnswer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1cd65",
   "metadata": {},
   "source": [
    "# 6. Chit-Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f00c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def chitChatAgent(state):\n",
    "    question = state['messages'][0].content\n",
    "\n",
    "    msg_prompt = f\"\"\"\n",
    "        You are a Chit-Chat Assistant.\n",
    "        Your task: reply politely when the context not related to  a user question so follow Instructions to senf a chit chat message.\n",
    "        \n",
    "        Instructions:\n",
    "        1. Start by apologizing that you don't fully understand the question.\n",
    "        2. Try to clarify by highlighting key words from the user's question.\n",
    "        3. Use short, simple sentences to suggest that the user rephrase their question.\n",
    "        5. shorts apologizing messages\n",
    "\n",
    "        User question: \"{question}\"\n",
    "        \"\"\"\n",
    "\n",
    "    prompt_msg_template = PromptTemplate(\n",
    "        template=msg_prompt,\n",
    "        input_variables=['question']\n",
    "    )\n",
    "\n",
    "    resonong = llm.invoke([{'role': 'user', 'content': prompt_msg_template.format(question=question)}])\n",
    "\n",
    "    return {'messages': [AIMessage(content=resonong.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "790dde26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize, but I'm not entirely sure I understand your question.\n",
      "Are you asking about \"Salaj\" and \"Zamilk\"?\n",
      "Could you please rephrase your question?\n",
      "Sorry for the confusion!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"where salaj play in zamilk\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_tool\",\n",
    "                        \"args\": {\"query\": \"classification of machine learning\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Supervised, Unsupervised, Reinforcement Learning\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = chitChatAgent(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f5d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f629be",
   "metadata": {},
   "source": [
    "# Work Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"DetectLangAgent\", detecting_language)\n",
    "workflow.add_node(\"Translate_Query\", TranslateQuery)\n",
    "workflow.add_node(\"RetriverAgent\", RetriverAgent)\n",
    "workflow.add_node(\"Chit-ChatAgent\", chitChatAgent)\n",
    "workflow.add_node(\"AnswerAgent\", GenerateAnswer)\n",
    "\n",
    "workflow.add_edge(START, \"DetectLangAgent\")\n",
    "workflow.add_edge(\"DetectLangAgent\", \"Translate_Query\")\n",
    "workflow.add_edge(\"Translate_Query\", \"RetriverAgent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"RetriverAgent\",\n",
    "    GraderDocumentAgent,\n",
    "    {\n",
    "        \"AnswerAgent\": \"AnswerAgent\",\n",
    "        \"Chit-ChatAgent\": \"Chit-ChatAgent\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"AnswerAgent\", END)\n",
    "workflow.add_edge(\"Chit-ChatAgent\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a594b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000027FCEF3DE50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e4189b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28600cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "******************************** اشرحلي encoder +++++++++++++++++++++++++\n",
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"اشرحلي encoder\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"\\n--- Node: {node} ---\")\n",
    "        for msg in update:\n",
    "            msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63f13dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "explain what is teanformer encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "explain what is teanformer encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "explain what is teanformer encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (6dcf51a7-bf27-43f3-bed0-0989d61da8dc)\n",
      " Call ID: 6dcf51a7-bf27-43f3-bed0-0989d61da8dc\n",
      "  Args:\n",
      "    query: transformer encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "E1+\n",
      "E2+\n",
      "E3+\n",
      "E4+\n",
      "E5+……………\n",
      "U\n",
      "U\n",
      "U\n",
      "U…logitslogitslogitslogitslogits\n",
      "Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\n",
      "get encoded, passed through a set of stacked transformer blocks, and then a language model\n",
      "head that predicts the next token.\n",
      "Fig. 9.1 sketches the transformer architecture. A transformer has three major\n",
      "components. At the center are columns of transformer blocks . Each block is a\n",
      "multilayer network (a multi-head attention layer, feedforward networks and layer\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "sampled so that their combined length was less than the 512 token input. Tokens\n",
      "within these sentence pairs were then masked using the MLM approach with the\n",
      "combined loss from the MLM and NSP objectives used for a ﬁnal loss. Because this\n",
      "ﬁnal loss is backpropagated through the entire transformer, the embeddings at each\n",
      "transformer layer will learn representations that are useful for predicting words from\n",
      "their neighbors. Since the [CLS] tokens are the direct input to the NSP classiﬁer,\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "A Transformer Encoder is a core component within the Transformer architecture, primarily responsible for processing an input sequence and generating rich, contextualized representations of each token within that sequence.\n",
      "\n",
      "Here's a breakdown based on the provided text:\n",
      "\n",
      "*   **Architecture:** A transformer generally has three major components. At its center are \"columns of transformer blocks.\" Each block is a multilayer network composed of a multi-head attention layer, feedforward networks, and layer normalization. These blocks are where the encoding magic happens.\n",
      "\n",
      "*   **Function:** The primary function of a transformer encoder is to \"encode\" each input token. This encoding process involves taking into account the \"entire input context\" to generate \"contextualized representations of input embeddings.\" This means that for each word in a sentence, the encoder doesn't just look at the word itself but also how it relates to all other words in the sentence.\n",
      "\n",
      "*   **Bidirectional Nature:** The text specifically mentions \"bidirectional encoders.\" This highlights a key feature: the encoder considers information from both directions (words before and words after) when creating a representation for a given word. This allows for a much deeper understanding of the word's meaning within its context.\n",
      "\n",
      "*   **Pretraining and Masked Language Model (MLM):** Transformer encoders are often used in \"pretrained language models.\" These models are trained using objectives like the \"masked language model (MLM) objective.\" In MLM, some words in the input are intentionally \"masked\" (hidden), and the model is trained to predict these missing words. This forces the encoder to learn robust contextual representations.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** While the original transformer had an \"encoder-decoder architecture,\" it's important to note that the decoder part was later used independently for causal language modeling. This implies that the encoder can function as a standalone component for tasks requiring sequence understanding and representation.\n",
      "\n",
      "In simpler terms, imagine a Transformer Encoder as a sophisticated reader. When it reads a sentence, it doesn't just read word by word in isolation. Instead, it reads the entire sentence, understanding how each word influences and is influenced by every other word. This deep understanding allows it to create a highly informative \"summary\" or \"representation\" for each word, considering its full context. These representations are then incredibly useful for various natural language processing tasks.\n",
      "******************************** اشرحلي encoder +++++++++++++++++++++++++\n",
      "\n",
      "--- Node: messages ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "اشرحلي encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (5f04b995-d3b7-48d4-9fd7-924de6a1d4c6)\n",
      " Call ID: 5f04b995-d3b7-48d4-9fd7-924de6a1d4c6\n",
      "  Args:\n",
      "    query: اشرحلي encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "tion performance is typically achieved with only minimal changes to the language\n",
      "model parameters, often limited to updates over the ﬁnal few layers of the trans-\n",
      "former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\n",
      "[CLS]entirelypredictableandlacksenergy\n",
      "Bidirectional Transformer EncoderhCLS\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+\n",
      "Ei+sentimentclassiﬁcation headWCy\n",
      "Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\n",
      "[CLS] token serves as input to a simple classiﬁer.\n",
      "11.4.2 Sequence-Pair Classiﬁcation\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "as well as all the parameters of the bidirectional encoder that are used to produce\n",
      "contextual embeddings for novel inputs.\n",
      "For many purposes, a pretrained multilingual model is more practical than a\n",
      "monolingual model, since it avoids the need to build many (a hundred!) separate\n",
      "monolingual models. And multilingual models can improve performance on low-\n",
      "resourced languages by leveraging linguistic information from a similar language in\n",
      "the training data that happens to have more resources. Nonetheless, when the num-\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "The provided text discusses encoders in the context of transformer models and natural language processing. Here's a breakdown of what it says about encoders:\n",
      "\n",
      "*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder's role is to process the input sequence and produce a meaningful representation.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\n",
      "\n",
      "*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\n",
      "\n",
      "*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\n",
      "\n",
      "*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\n",
      "\n",
      "In essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "explain what is teanformer encoder\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain to me encoder\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "Tool Calls:\n",
      "  retriever_tool (6dcf51a7-bf27-43f3-bed0-0989d61da8dc)\n",
      " Call ID: 6dcf51a7-bf27-43f3-bed0-0989d61da8dc\n",
      "  Args:\n",
      "    query: transformer encoder\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_tool\n",
      "\n",
      "E1+\n",
      "E2+\n",
      "E3+\n",
      "E4+\n",
      "E5+……………\n",
      "U\n",
      "U\n",
      "U\n",
      "U…logitslogitslogitslogitslogits\n",
      "Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\n",
      "get encoded, passed through a set of stacked transformer blocks, and then a language model\n",
      "head that predicts the next token.\n",
      "Fig. 9.1 sketches the transformer architecture. A transformer has three major\n",
      "components. At the center are columns of transformer blocks . Each block is a\n",
      "multilayer network (a multi-head attention layer, feedforward networks and layer\n",
      "\n",
      "11.6 Summary\n",
      "This chapter has introduced the bidirectional encoder and the masked language\n",
      "model . Here’s a summary of the main points that we covered:\n",
      "• Bidirectional encoders can be used to generate contextualized representations\n",
      "of input embeddings using the entire input context.\n",
      "• Pretrained language models based on bidirectional encoders can be learned\n",
      "using a masked language model objective where a model is trained to guess\n",
      "the missing information from an input.\n",
      "• The vector output of each transformer block or component in a particular to-\n",
      "\n",
      "model18 CHAPTER 9 • T HETRANSFORMER\n",
      "decoder model for transformers that we’ll see how to apply to machine translation\n",
      "in Chapter 13. (Confusingly, the original introduction of the transformer had an\n",
      "encoder-decoder architecture, and it was only later that the standard paradigm for\n",
      "causal language model was deﬁned by using only the decoder part of this original\n",
      "architecture).\n",
      "9.6 Summary\n",
      "This chapter has introduced the transformer and its components for the task of lan-\n",
      "guage modeling. We’ll continue the task of language modeling including issues like\n",
      "training and sampling in the next chapter.\n",
      "\n",
      "sampled so that their combined length was less than the 512 token input. Tokens\n",
      "within these sentence pairs were then masked using the MLM approach with the\n",
      "combined loss from the MLM and NSP objectives used for a ﬁnal loss. Because this\n",
      "ﬁnal loss is backpropagated through the entire transformer, the embeddings at each\n",
      "transformer layer will learn representations that are useful for predicting words from\n",
      "their neighbors. Since the [CLS] tokens are the direct input to the NSP classiﬁer,\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: RetriverAgent\n",
      "\n",
      "A Transformer Encoder is a core component within the Transformer architecture, primarily responsible for processing an input sequence and generating rich, contextualized representations of each token within that sequence.\n",
      "\n",
      "Here's a breakdown based on the provided text:\n",
      "\n",
      "*   **Architecture:** A transformer generally has three major components. At its center are \"columns of transformer blocks.\" Each block is a multilayer network composed of a multi-head attention layer, feedforward networks, and layer normalization. These blocks are where the encoding magic happens.\n",
      "\n",
      "*   **Function:** The primary function of a transformer encoder is to \"encode\" each input token. This encoding process involves taking into account the \"entire input context\" to generate \"contextualized representations of input embeddings.\" This means that for each word in a sentence, the encoder doesn't just look at the word itself but also how it relates to all other words in the sentence.\n",
      "\n",
      "*   **Bidirectional Nature:** The text specifically mentions \"bidirectional encoders.\" This highlights a key feature: the encoder considers information from both directions (words before and words after) when creating a representation for a given word. This allows for a much deeper understanding of the word's meaning within its context.\n",
      "\n",
      "*   **Pretraining and Masked Language Model (MLM):** Transformer encoders are often used in \"pretrained language models.\" These models are trained using objectives like the \"masked language model (MLM) objective.\" In MLM, some words in the input are intentionally \"masked\" (hidden), and the model is trained to predict these missing words. This forces the encoder to learn robust contextual representations.\n",
      "\n",
      "*   **Encoder-Decoder Architecture:** While the original transformer had an \"encoder-decoder architecture,\" it's important to note that the decoder part was later used independently for causal language modeling. This implies that the encoder can function as a standalone component for tasks requiring sequence understanding and representation.\n",
      "\n",
      "In simpler terms, imagine a Transformer Encoder as a sophisticated reader. When it reads a sentence, it doesn't just read word by word in isolation. Instead, it reads the entire sentence, understanding how each word influences and is influenced by every other word. This deep understanding allows it to create a highly informative \"summary\" or \"representation\" for each word, considering its full context. These representations are then incredibly useful for various natural language processing tasks.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A Transformer Encoder is a core component within the Transformer architecture responsible for processing an input sequence and generating rich, contextualized representations for each token (like a word) in that sequence.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **Function:** Its main job is to \"encode\" each input token by considering the **entire input context**. This means it doesn't just look at a word in isolation, but how it relates to all other words in the sentence, both before and after it.\n",
      "*   **Bidirectional:** It's often \"bidirectional,\" meaning it considers information from both directions (words preceding and following) to create a deep understanding of each word's meaning within its context.\n",
      "*   **Architecture:** It consists of \"transformer blocks,\" which are multilayer networks composed of multi-head attention layers, feedforward networks, and layer normalization. These components work together to perform the encoding.\n",
      "*   **Pretraining:** Transformer encoders are frequently used in \"pretrained language models,\" where they are trained using objectives like the \"masked language model (MLM) objective.\" In MLM, some words are hidden, and the encoder learns to predict them, forcing it to develop robust contextual representations.\n",
      "\n",
      "In simple terms, imagine it as a sophisticated reader that understands a sentence by looking at all the words together, not just one by one. It creates a highly informative \"summary\" or \"representation\" for each word, taking into account its full context.\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"explain what is teanformer encoder\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"\\n--- Node: {node} ---\")\n",
    "        for msg in update:\n",
    "            msg.pretty_print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba2369e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************** اشرحلي encoder +++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='اشرحلي encoder', additional_kwargs={}, response_metadata={}, id='021a0399-9cf3-4da0-a687-ca81226e9dec'),\n",
       "  HumanMessage(content='Explain to me encoder', additional_kwargs={}, response_metadata={}, id='9d929ba6-868d-4ce5-b7f0-9c98682b5ffd'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"\\\\u0627\\\\u0634\\\\u0631\\\\u062d\\\\u0644\\\\u064a encoder\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--303698d2-e5a4-4bc6-8aa6-1d4c6dcdf236-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'اشرحلي encoder'}, 'id': '5f04b995-d3b7-48d4-9fd7-924de6a1d4c6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 124, 'output_tokens': 82, 'total_tokens': 206, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 63}}),\n",
       "  ToolMessage(content='tion performance is typically achieved with only minimal changes to the language\\nmodel parameters, often limited to updates over the ﬁnal few layers of the trans-\\nformer. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.\\n[CLS]entirelypredictableandlacksenergy\\nBidirectional Transformer EncoderhCLS\\nEi+\\nEi+\\nEi+\\nEi+\\nEi+\\nEi+sentimentclassiﬁcation headWCy\\nFigure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the\\n[CLS] token serves as input to a simple classiﬁer.\\n11.4.2 Sequence-Pair Classiﬁcation\\n\\nmodel18 CHAPTER 9 • T HETRANSFORMER\\ndecoder model for transformers that we’ll see how to apply to machine translation\\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\\nencoder-decoder architecture, and it was only later that the standard paradigm for\\ncausal language model was deﬁned by using only the decoder part of this original\\narchitecture).\\n9.6 Summary\\nThis chapter has introduced the transformer and its components for the task of lan-\\nguage modeling. We’ll continue the task of language modeling including issues like\\ntraining and sampling in the next chapter.\\n\\n11.6 Summary\\nThis chapter has introduced the bidirectional encoder and the masked language\\nmodel . Here’s a summary of the main points that we covered:\\n• Bidirectional encoders can be used to generate contextualized representations\\nof input embeddings using the entire input context.\\n• Pretrained language models based on bidirectional encoders can be learned\\nusing a masked language model objective where a model is trained to guess\\nthe missing information from an input.\\n• The vector output of each transformer block or component in a particular to-\\n\\nas well as all the parameters of the bidirectional encoder that are used to produce\\ncontextual embeddings for novel inputs.\\nFor many purposes, a pretrained multilingual model is more practical than a\\nmonolingual model, since it avoids the need to build many (a hundred!) separate\\nmonolingual models. And multilingual models can improve performance on low-\\nresourced languages by leveraging linguistic information from a similar language in\\nthe training data that happens to have more resources. Nonetheless, when the num-', name='retriever_tool', id='18b75804-ef8d-4f41-942e-01febe3bd610', tool_call_id='5f04b995-d3b7-48d4-9fd7-924de6a1d4c6'),\n",
       "  AIMessage(content='The provided text discusses encoders in the context of transformer models and natural language processing. Here\\'s a breakdown of what it says about encoders:\\n\\n*   **Bidirectional Transformer Encoder:** This is a key component for sequence classification. The output vector for a special \"[CLS]\" token (likely representing the entire input sequence) from this encoder serves as input to a simple classifier. This suggests the encoder\\'s role is to process the input sequence and produce a meaningful representation.\\n\\n*   **Encoder-Decoder Architecture:** The original transformer model had an encoder-decoder architecture. However, later, the standard paradigm for causal language models was defined by using only the decoder part of this original architecture. This implies that encoders are a distinct part of a larger transformer system, and their use can vary depending on the specific task.\\n\\n*   **Bidirectional Encoder and Masked Language Model:** Bidirectional encoders are used to generate \"contextualized representations of input embeddings\" by considering the entire input context. This means they look at words in relation to all other words in the input. These encoders can be pretrained using a \"masked language model objective,\" where the model is trained to predict missing information from an input.\\n\\n*   **Output of Transformer Block/Component:** The vector output of each transformer block or component in a particular token (which would be part of the encoder) is important.\\n\\n*   **Parameters of Bidirectional Encoder:** The parameters of the bidirectional encoder are used to produce contextual embeddings for new inputs.\\n\\nIn essence, an encoder, particularly a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. These representations are then used for various downstream tasks like classification or as input to other parts of a larger model (like a decoder). The \"bidirectional\" aspect means it considers both preceding and succeeding words to understand the full context of each word.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--03f0cbe6-904a-4219-9f5d-dc54a4375600-0', usage_metadata={'input_tokens': 675, 'output_tokens': 394, 'total_tokens': 1069, 'input_token_details': {'cache_read': 0}}),\n",
       "  AIMessage(content=\"An encoder, especially a bidirectional transformer encoder, is a component that processes an input sequence (like a sentence) to create rich, contextualized representations of the words or tokens within that sequence. It considers the entire input context (both preceding and succeeding words) to understand each word's full meaning. These representations are then used for various tasks, such as classification or as input to other parts of a larger model.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ec992296-41e4-40f9-82d2-d7c9560b2f5f-0', usage_metadata={'input_tokens': 450, 'output_tokens': 257, 'total_tokens': 707, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 174}}),\n",
       "  HumanMessage(content='explain what is teanformer encoder', additional_kwargs={}, response_metadata={}, id='56dff758-5a52-4fa5-96ba-a72de21b8ea1'),\n",
       "  HumanMessage(content='Explain to me encoder', additional_kwargs={}, response_metadata={}, id='92da127f-7b02-4268-8720-3439a70a41f8'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"transformer encoder\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--548816a4-4906-4f59-8020-b8aa40aae24b-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'transformer encoder'}, 'id': '6dcf51a7-bf27-43f3-bed0-0989d61da8dc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1167, 'output_tokens': 46, 'total_tokens': 1213, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 29}}),\n",
       "  ToolMessage(content='E1+\\nE2+\\nE3+\\nE4+\\nE5+……………\\nU\\nU\\nU\\nU…logitslogitslogitslogitslogits\\nFigure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\\nget encoded, passed through a set of stacked transformer blocks, and then a language model\\nhead that predicts the next token.\\nFig. 9.1 sketches the transformer architecture. A transformer has three major\\ncomponents. At the center are columns of transformer blocks . Each block is a\\nmultilayer network (a multi-head attention layer, feedforward networks and layer\\n\\n11.6 Summary\\nThis chapter has introduced the bidirectional encoder and the masked language\\nmodel . Here’s a summary of the main points that we covered:\\n• Bidirectional encoders can be used to generate contextualized representations\\nof input embeddings using the entire input context.\\n• Pretrained language models based on bidirectional encoders can be learned\\nusing a masked language model objective where a model is trained to guess\\nthe missing information from an input.\\n• The vector output of each transformer block or component in a particular to-\\n\\nmodel18 CHAPTER 9 • T HETRANSFORMER\\ndecoder model for transformers that we’ll see how to apply to machine translation\\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\\nencoder-decoder architecture, and it was only later that the standard paradigm for\\ncausal language model was deﬁned by using only the decoder part of this original\\narchitecture).\\n9.6 Summary\\nThis chapter has introduced the transformer and its components for the task of lan-\\nguage modeling. We’ll continue the task of language modeling including issues like\\ntraining and sampling in the next chapter.\\n\\nsampled so that their combined length was less than the 512 token input. Tokens\\nwithin these sentence pairs were then masked using the MLM approach with the\\ncombined loss from the MLM and NSP objectives used for a ﬁnal loss. Because this\\nﬁnal loss is backpropagated through the entire transformer, the embeddings at each\\ntransformer layer will learn representations that are useful for predicting words from\\ntheir neighbors. Since the [CLS] tokens are the direct input to the NSP classiﬁer,', name='retriever_tool', id='d38267a2-7ead-49d4-98cf-6d5c723b2164', tool_call_id='6dcf51a7-bf27-43f3-bed0-0989d61da8dc'),\n",
       "  AIMessage(content='A Transformer Encoder is a core component within the Transformer architecture, primarily responsible for processing an input sequence and generating rich, contextualized representations of each token within that sequence.\\n\\nHere\\'s a breakdown based on the provided text:\\n\\n*   **Architecture:** A transformer generally has three major components. At its center are \"columns of transformer blocks.\" Each block is a multilayer network composed of a multi-head attention layer, feedforward networks, and layer normalization. These blocks are where the encoding magic happens.\\n\\n*   **Function:** The primary function of a transformer encoder is to \"encode\" each input token. This encoding process involves taking into account the \"entire input context\" to generate \"contextualized representations of input embeddings.\" This means that for each word in a sentence, the encoder doesn\\'t just look at the word itself but also how it relates to all other words in the sentence.\\n\\n*   **Bidirectional Nature:** The text specifically mentions \"bidirectional encoders.\" This highlights a key feature: the encoder considers information from both directions (words before and words after) when creating a representation for a given word. This allows for a much deeper understanding of the word\\'s meaning within its context.\\n\\n*   **Pretraining and Masked Language Model (MLM):** Transformer encoders are often used in \"pretrained language models.\" These models are trained using objectives like the \"masked language model (MLM) objective.\" In MLM, some words in the input are intentionally \"masked\" (hidden), and the model is trained to predict these missing words. This forces the encoder to learn robust contextual representations.\\n\\n*   **Encoder-Decoder Architecture:** While the original transformer had an \"encoder-decoder architecture,\" it\\'s important to note that the decoder part was later used independently for causal language modeling. This implies that the encoder can function as a standalone component for tasks requiring sequence understanding and representation.\\n\\nIn simpler terms, imagine a Transformer Encoder as a sophisticated reader. When it reads a sentence, it doesn\\'t just read word by word in isolation. Instead, it reads the entire sentence, understanding how each word influences and is influenced by every other word. This deep understanding allows it to create a highly informative \"summary\" or \"representation\" for each word, considering its full context. These representations are then incredibly useful for various natural language processing tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--1051e93c-76ed-43b0-9d27-c4b6de758ffb-0', usage_metadata={'input_tokens': 1712, 'output_tokens': 477, 'total_tokens': 2189, 'input_token_details': {'cache_read': 812}}),\n",
       "  AIMessage(content='A Transformer Encoder is a core component within the Transformer architecture responsible for processing an input sequence and generating rich, contextualized representations for each token (like a word) in that sequence.\\n\\nHere\\'s a breakdown:\\n\\n*   **Function:** Its main job is to \"encode\" each input token by considering the **entire input context**. This means it doesn\\'t just look at a word in isolation, but how it relates to all other words in the sentence, both before and after it.\\n*   **Bidirectional:** It\\'s often \"bidirectional,\" meaning it considers information from both directions (words preceding and following) to create a deep understanding of each word\\'s meaning within its context.\\n*   **Architecture:** It consists of \"transformer blocks,\" which are multilayer networks composed of multi-head attention layers, feedforward networks, and layer normalization. These components work together to perform the encoding.\\n*   **Pretraining:** Transformer encoders are frequently used in \"pretrained language models,\" where they are trained using objectives like the \"masked language model (MLM) objective.\" In MLM, some words are hidden, and the encoder learns to predict them, forcing it to develop robust contextual representations.\\n\\nIn simple terms, imagine it as a sophisticated reader that understands a sentence by looking at all the words together, not just one by one. It creates a highly informative \"summary\" or \"representation\" for each word, taking into account its full context.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b83bde8f-dcc6-4bc9-b307-1460c96bb15d-0', usage_metadata={'input_tokens': 533, 'output_tokens': 479, 'total_tokens': 1012, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 182}}),\n",
       "  HumanMessage(content='Which ML type is actually better?', additional_kwargs={}, response_metadata={}, id='8c093e4b-3d1b-4dde-a6ee-f0d0f8da9088'),\n",
       "  HumanMessage(content='Explain to me encoder', additional_kwargs={}, response_metadata={}, id='23a059f2-fe9c-4d90-86d6-8c7b8b7a516d'),\n",
       "  AIMessage(content='There isn\\'t a single \"better\" type of machine learning (ML). The best ML type depends entirely on the specific problem you\\'re trying to solve, the nature of your data, the resources you have, and the desired outcome.\\n\\nFor example:\\n*   **Supervised learning** (like classification or regression) is great when you have labeled data and want to predict a specific output.\\n*   **Unsupervised learning** (like clustering or dimensionality reduction) is useful for finding patterns and structures in unlabeled data.\\n*   **Reinforcement learning** is ideal for tasks where an agent learns through trial and error in an environment, like game playing or robotics.\\n\\nEach type has its strengths and weaknesses, and often, a combination of techniques or a hybrid approach might be the most effective.\\n\\nRegarding your second question, \"Explain to me encoder\":', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"explain to me encoder\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--00262fd0-e3ee-41ad-8af2-9e3f95c85439-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'explain to me encoder'}, 'id': 'f892ceb7-39d1-4a46-bffa-d592139cfc2c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2501, 'output_tokens': 410, 'total_tokens': 2911, 'input_token_details': {'cache_read': 1735}, 'output_token_details': {'reasoning': 214}}),\n",
       "  ToolMessage(content='tokens. Bidirectional encoders use self-attention to map sequences of input embed-\\ndings (x1; :::;xn)to sequences of output embeddings of the same length (h1; :::;hn),\\nwhere the output vectors have been contextualized using information from the en-\\ntire input sequence. These output embeddings are contextualized representations of\\neach input token that are useful across a range of applications where we need to do\\na classiﬁcation or a decision based on the token in context.\\nRemember that we said the models of Chapter 9 are sometimes called decoder-\\n\\n11.6 Summary\\nThis chapter has introduced the bidirectional encoder and the masked language\\nmodel . Here’s a summary of the main points that we covered:\\n• Bidirectional encoders can be used to generate contextualized representations\\nof input embeddings using the entire input context.\\n• Pretrained language models based on bidirectional encoders can be learned\\nusing a masked language model objective where a model is trained to guess\\nthe missing information from an input.\\n• The vector output of each transformer block or component in a particular to-\\n\\nof input words to inform a generative decoder (see Chapter 13) was developed by\\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\\nfor MT. This idea was extended to self-attention by dropping the need for separate\\nencoding and decoding sequences and instead seeing attention as a way of weighting\\nthe tokens in collecting information passed from lower layers to higher layers (Ling\\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\\nOther aspects of the transformer, including the terminology of key, query, and\\n\\nthe vocabulary. By contrast, with contextual embeddings, such as those learned by\\nmasked language models like BERT, each word wwill be represented by a different\\nvector each time it appears in a different context. While the causal language models\\nof Chapter 9 also use contextual embeddings, the embeddings created by masked\\nlanguage models seem to function particularly well as representations.\\n11.1 Bidirectional Transformer Encoders\\nLet’s begin by introducing the bidirectional transformer encoder that underlies mod-\\nels like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT', name='retriever_tool', id='1f1ecdb9-31f9-4660-9dfc-a1dfbdabef38', tool_call_id='f892ceb7-39d1-4a46-bffa-d592139cfc2c'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"Explain to me encoder\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--276701d7-8c10-43d2-b45c-312d0f3b1db8-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'Which ML type is actually better?'}, 'id': 'f9d40b7a-e97a-4ce5-99f7-2ce0ec384e3c', 'type': 'tool_call'}, {'name': 'retriever_tool', 'args': {'query': 'Explain to me encoder'}, 'id': '31d6419a-d097-4f1f-9ec2-1863b675aad4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3045, 'output_tokens': 1609, 'total_tokens': 4654, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1568}}),\n",
       "  ToolMessage(content='accurate on a training set. So given some feat ures, X, and some correct prices, Y, we \\nmight want to make that thet a square difference between th e prediction of the algorithm \\nand the actual price [inaudible].  \\nSo to choose parameters theta, unless we wa nt to minimize over the parameters theta, so \\nthe squared area between the pred icted price and the actual pri ce. And so going to fill this \\nin. We have M training examples. So the sum from I equals one through M of my M \\ntraining examples, of price predicted on the It h house in my training set. Mine is the\\n\\nout that it is often much easier to maximize the log of the likelihood rather than maximize \\nthe likelihood. So the log likelihood L of theta is just log of capital L. This will, therefore, \\nbe sum of this. Okay? And so to fit the para meters theta of our model we’ll find the value \\nof theta that maximizes this log likelihood. Yeah?  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Say that again.  Student: YI is [inaudible].  \\nInstructor (Andrew Ng) :Oh, yes. Thanks. So having maximized this function – well, it\\n\\nparameters, how do you estimate the parameters theta? So given a training set, what \\nparameters theta do you want to choose for your model? Well, the principle of maximum \\nlikelihood estimation says that, right? You can choose the valu e of theta that makes the \\ndata as probable as possible, right? So choose theta to maximize the likelihood. Or in \\nother words choose the parameters that make th e data as probable as possible, right? So \\nthis is massive likely your estimation from si x to six. So it’s choose the parameters that\\n\\nGaussian. So that no matter what sigma squared is since it’s a positive number the value of theta we end up with will be the same, right? So because minimizing this you get the \\nsame value of theta no matter what sigma squared is. So it’s as if in this model the value \\nof sigma squared doesn’t really  matter. Just remember that for the next lecture. We’ll \\ncome back to this again. Any questions a bout this? Actually, let me clean up another \\ncouple of boards and then I’ll see what questions you have.  \\nOkay. Any questions? Yeah?', name='retriever_tool', id='26136cac-2a9c-4240-90bc-577df048d1a1', tool_call_id='f9d40b7a-e97a-4ce5-99f7-2ce0ec384e3c'),\n",
       "  ToolMessage(content='tokens. Bidirectional encoders use self-attention to map sequences of input embed-\\ndings (x1; :::;xn)to sequences of output embeddings of the same length (h1; :::;hn),\\nwhere the output vectors have been contextualized using information from the en-\\ntire input sequence. These output embeddings are contextualized representations of\\neach input token that are useful across a range of applications where we need to do\\na classiﬁcation or a decision based on the token in context.\\nRemember that we said the models of Chapter 9 are sometimes called decoder-\\n\\n11.6 Summary\\nThis chapter has introduced the bidirectional encoder and the masked language\\nmodel . Here’s a summary of the main points that we covered:\\n• Bidirectional encoders can be used to generate contextualized representations\\nof input embeddings using the entire input context.\\n• Pretrained language models based on bidirectional encoders can be learned\\nusing a masked language model objective where a model is trained to guess\\nthe missing information from an input.\\n• The vector output of each transformer block or component in a particular to-\\n\\nof input words to inform a generative decoder (see Chapter 13) was developed by\\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\\nfor MT. This idea was extended to self-attention by dropping the need for separate\\nencoding and decoding sequences and instead seeing attention as a way of weighting\\nthe tokens in collecting information passed from lower layers to higher layers (Ling\\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\\nOther aspects of the transformer, including the terminology of key, query, and\\n\\nthe vocabulary. By contrast, with contextual embeddings, such as those learned by\\nmasked language models like BERT, each word wwill be represented by a different\\nvector each time it appears in a different context. While the causal language models\\nof Chapter 9 also use contextual embeddings, the embeddings created by masked\\nlanguage models seem to function particularly well as representations.\\n11.1 Bidirectional Transformer Encoders\\nLet’s begin by introducing the bidirectional transformer encoder that underlies mod-\\nels like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT', name='retriever_tool', id='ba4a7f96-1504-4cd0-8220-60ead24e8344', tool_call_id='31d6419a-d097-4f1f-9ec2-1863b675aad4'),\n",
       "  AIMessage(content='It seems like you\\'ve asked two separate questions: \"Which ML type is actually better?\" and \"Explain to me encoder.\" I\\'ll address them separately.\\n\\n**Regarding \"Which ML type is actually better?\"**\\n\\nThe provided text does not directly answer which ML type is \"better.\" It discusses concepts related to fitting model parameters, maximizing likelihood, and minimizing squared differences in the context of machine learning. These are general principles for optimizing models, not a comparison of different ML types.\\n\\nTherefore, based on the provided text, I cannot tell you which ML type is \"better.\" The \"best\" ML type depends entirely on the specific problem you\\'re trying to solve, the nature of your data, and your desired outcome.\\n\\n**Regarding \"Explain to me encoder\"**\\n\\nAn encoder, particularly a **bidirectional transformer encoder**, is a fundamental component in modern natural language processing (NLP) models. Here\\'s what the text tells us:\\n\\n*   **Purpose:** Encoders are used to generate **contextualized representations** of input embeddings. This means they take a sequence of input tokens (like words in a sentence) and transform them into a sequence of output embeddings, where each output embedding has been enriched with information from the *entire* input sequence.\\n*   **How it works:** Bidirectional encoders use **self-attention** to achieve this. Self-attention allows the model to weigh the importance of different tokens in the input sequence when creating the representation for any given token. This \"bidirectional\" aspect means it considers both the words that come *before* and *after* a given word to fully understand its context.\\n*   **Applications:** These contextualized representations are highly useful across a range of applications where you need to make a classification or decision based on a token\\'s meaning within its context.\\n*   **Pretraining with Masked Language Models (MLM):** Pretrained language models often use bidirectional encoders trained with a **masked language model objective**. In this setup, some input tokens are intentionally \"masked\" (hidden), and the encoder is trained to predict the missing information. This forces the encoder to learn robust and context-aware representations. Models like BERT and its descendants (RoBERTa, SpanBERT) are built upon this concept.\\n*   **Contrast with Non-Contextual Embeddings:** The text highlights that with contextual embeddings (like those learned by masked language models), each word will be represented by a different vector each time it appears in a different context. This is in contrast to earlier methods where a word might have a single, fixed representation regardless of its surrounding words. The text suggests that embeddings created by masked language models \"seem to function particularly well as representations.\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--c7ed1c97-e503-4bc5-bf66-984a87d7f89e-0', usage_metadata={'input_tokens': 4160, 'output_tokens': 556, 'total_tokens': 4716, 'input_token_details': {'cache_read': 2755}}),\n",
       "  AIMessage(content='An encoder, especially a **bidirectional transformer encoder**, is a key component in modern natural language processing (NLP).\\n\\nHere\\'s an explanation based on the provided text:\\n\\n*   **Purpose:** Encoders generate **contextualized representations** of input embeddings. This means they take a sequence of input tokens (like words in a sentence) and transform them into output embeddings where each output embedding contains information from the *entire* input sequence.\\n*   **How it works:** Bidirectional encoders use **self-attention**. This mechanism allows the model to weigh the importance of different tokens in the input sequence when creating a representation for any specific token. \"Bidirectional\" means it considers both preceding and succeeding words to understand a word\\'s full context.\\n*   **Applications:** The contextualized representations produced by encoders are useful for various applications where decisions or classifications need to be made based on a token\\'s meaning within its context.\\n*   **Pretraining:** These encoders are often pretrained using a **masked language model (MLM) objective**. In this method, some input tokens are intentionally masked (hidden), and the encoder is trained to predict the missing information. This forces the encoder to learn strong, context-aware representations. Models like BERT and its variations (RoBERTa, SpanBERT) are built on this principle.\\n*   **Contrast with Non-Contextual Embeddings:** Unlike older methods where a word might have a single, fixed representation, contextual embeddings from masked language models mean that a word will have a different vector representation each time it appears in a different context. These contextual embeddings are noted to \"seem to function particularly well as representations.\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--620e4a1a-6d85-43d2-a472-d3fba242db6b-0', usage_metadata={'input_tokens': 612, 'output_tokens': 440, 'total_tokens': 1052, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 101}})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = graph.invoke( {\"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Which ML type is actually better?\",\n",
    "            }\n",
    "        ]} ,  config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ce4c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************** types of manchine learning +++++++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='types of manchine learning', additional_kwargs={}, response_metadata={}, id='f4275b84-2c34-4d2b-bfdb-a1264b167956'),\n",
       "  HumanMessage(content='types of manchine learning', additional_kwargs={}, response_metadata={}, id='6b3bbcc1-9e08-450f-9a68-6b62d51c1f25'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"types of manchine learningtypes of manchine learning\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--31691553-3232-4597-855f-a3919091497b-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'types of manchine learningtypes of manchine learning'}, 'id': '28606d6e-9931-4ed7-9017-fea506945d66', 'type': 'tool_call'}], usage_metadata={'input_tokens': 126, 'output_tokens': 73, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 48}}),\n",
       "  ToolMessage(content=\"the requirements of a downstream language understanding task. This aspect of the\\npretrain-ﬁnetune paradigm is an instance of what is called transfer learning in ma-transfer\\nlearning\\nchine learning: the method of acquiring knowledge from one task or domain, and\\nthen applying it (transferring it) to solve a new task.\\nThe second idea that we introduce in this chapter is the idea of contextual em-\\nbeddings : representations for words in context. The methods of Chapter 6 like\\nword2vec or GloVe learned a single vector embedding for each unique word win\\n\\nhow to use them well. And I've noticed this  is something that really not many other classes teach. And this is something I'm rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet's see. So [inaudible] the board. After lear ning theory, there's a nother class of learning \\nalgorithms that I then want to teach you a bout, and that's unsupervised learning. So you\\n\\nwere called. He’s using it to explore the myster ious problem of how the brain learns. This \\nperceptron is being trained to recognize the difference between males and females. It is \\nsomething that all of us can do easily, but fe w of us can explain how. To get a computer \\nto do this it would involve working out ma ny complex rules about faces and writing a \\ncomputer program, but this perceptron was si mply given lots and lots of examples, \\nincluding some with unus ual hairstyles. But when it comes to a beetle the computer looks\\n\\norders? Or would th eta [inaudible]?  \\nInstructor (Andrew Ng) :All great questions. The answer – so the question was, is this a \\ntypical hypothesis or can theta be a function of other variables and so on. And the answer \\nis sort of yes. For now, just for this first learning algorithm we'll talk about using a linear \\nhypothesis class. A little bit ac tually later this quarter, we'll talk about much more \\ncomplicated hypothesis classes, and we'll actua lly talk about higher order functions as \\nwell, a little bit later today.\", name='retriever_tool', id='72850e77-6d60-4c50-b7d3-eaecefe97002', tool_call_id='28606d6e-9931-4ed7-9017-fea506945d66'),\n",
       "  AIMessage(content=\"the requirements of a downstream language understanding task. This aspect of the\\\\npretrain-ﬁnetune paradigm is an instance of what is called transfer learning in ma-transfer\\\\nlearning\\\\nchine learning: the method of acquiring knowledge from one task or domain, and\\\\nthen applying it (transferring it) to solve a new task.\\\\nThe second idea that we introduce in this chapter is the idea of contextual em-\\\\nbeddings : representations for words in context. The methods of Chapter 6 like\\\\nword2vec or GloVe learned a single vector embedding for each unique word win\\\\n\\\\nhow to use them well. And I\\\\'ve noticed this is something that really not many other classes teach. And this is something I\\\\'m rea lly convinced is a huge deal, and so by the \\\\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\\\nreally good at applying these learning algor ithms and getting them to work amazingly \\\\nwell in many problems. Okay? \\\\nLet\\\\'s see. So [inaudible] the board. After lear ning theory, there\\\\'s a nother class of learning \\\\nalgorithms that I then want to teach you a bout, and that\\\\'s unsupervised learning. So you\\\\n\\\\nwere called. He’s using it to explore the myster ious problem of how the brain learns. This \\\\nperceptron is being trained to recognize the difference between males and females. It is \\\\nsomething that all of us can do easily, but fe w of us can explain how. To get a computer \\\\nto do this it would involve working out ma ny complex rules about faces and writing a \\\\ncomputer program, but this perceptron was si mply given lots and lots of examples, \\\\nincluding some with unus ual hairstyles. But when it comes to a beetle the computer looks\\\\n\\\\norders? Or would th eta [inaudible]? \\\\nInstructor (Andrew Ng) :All great questions. The answer – so the question was, is this a \\\\ntypical hypothesis or can theta be a function of other variables and so on. And the answer \\\\nis sort of yes. For now, just for this first learning algorithm we\\\\'ll talk about using a linear \\\\nhypothesis class. A little bit ac tually later this quarter, we\\\\'ll talk about much more \\\\ncomplicated hypothesis classes, and we\\\\'ll actua lly talk about higher order functions as \\\\nwell, a little bit later today.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--c6acc751-d97b-4ca1-8d45-59642f62fe78-0', usage_metadata={'input_tokens': 702, 'output_tokens': 533, 'total_tokens': 1235, 'input_token_details': {'cache_read': 0}}),\n",
       "  AIMessage(content='Based on the context provided, the types of machine learning mentioned are:\\n\\n*   **Transfer learning**\\n*   **Unsupervised learning**\\n\\nThe context also describes a perceptron being trained with examples to recognize differences, which is characteristic of **supervised learning**, though the term itself is not explicitly used.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d1fdc398-c605-4e55-a894-acc7649d8b6a-0', usage_metadata={'input_tokens': 590, 'output_tokens': 480, 'total_tokens': 1070, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 418}})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "output = graph.invoke( {\"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"types of manchine learning\",\n",
    "            }\n",
    "        ]} ,  config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55d548cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the context provided, the types of machine learning mentioned are:\n",
      "\n",
      "*   **Transfer learning**\n",
      "*   **Unsupervised learning**\n",
      "\n",
      "The context also describes a perceptron being trained with examples to recognize differences, which is characteristic of **supervised learning**, though the term itself is not explicitly used.\n"
     ]
    }
   ],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92bed2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='types of manchine learning', additional_kwargs={}, response_metadata={}, id='f4275b84-2c34-4d2b-bfdb-a1264b167956'),\n",
       "  HumanMessage(content='types of manchine learning', additional_kwargs={}, response_metadata={}, id='6b3bbcc1-9e08-450f-9a68-6b62d51c1f25'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"types of manchine learningtypes of manchine learning\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--31691553-3232-4597-855f-a3919091497b-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'types of manchine learningtypes of manchine learning'}, 'id': '28606d6e-9931-4ed7-9017-fea506945d66', 'type': 'tool_call'}], usage_metadata={'input_tokens': 126, 'output_tokens': 73, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 48}}),\n",
       "  ToolMessage(content=\"the requirements of a downstream language understanding task. This aspect of the\\npretrain-ﬁnetune paradigm is an instance of what is called transfer learning in ma-transfer\\nlearning\\nchine learning: the method of acquiring knowledge from one task or domain, and\\nthen applying it (transferring it) to solve a new task.\\nThe second idea that we introduce in this chapter is the idea of contextual em-\\nbeddings : representations for words in context. The methods of Chapter 6 like\\nword2vec or GloVe learned a single vector embedding for each unique word win\\n\\nhow to use them well. And I've noticed this  is something that really not many other classes teach. And this is something I'm rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet's see. So [inaudible] the board. After lear ning theory, there's a nother class of learning \\nalgorithms that I then want to teach you a bout, and that's unsupervised learning. So you\\n\\nwere called. He’s using it to explore the myster ious problem of how the brain learns. This \\nperceptron is being trained to recognize the difference between males and females. It is \\nsomething that all of us can do easily, but fe w of us can explain how. To get a computer \\nto do this it would involve working out ma ny complex rules about faces and writing a \\ncomputer program, but this perceptron was si mply given lots and lots of examples, \\nincluding some with unus ual hairstyles. But when it comes to a beetle the computer looks\\n\\norders? Or would th eta [inaudible]?  \\nInstructor (Andrew Ng) :All great questions. The answer – so the question was, is this a \\ntypical hypothesis or can theta be a function of other variables and so on. And the answer \\nis sort of yes. For now, just for this first learning algorithm we'll talk about using a linear \\nhypothesis class. A little bit ac tually later this quarter, we'll talk about much more \\ncomplicated hypothesis classes, and we'll actua lly talk about higher order functions as \\nwell, a little bit later today.\", name='retriever_tool', id='72850e77-6d60-4c50-b7d3-eaecefe97002', tool_call_id='28606d6e-9931-4ed7-9017-fea506945d66'),\n",
       "  AIMessage(content=\"the requirements of a downstream language understanding task. This aspect of the\\\\npretrain-ﬁnetune paradigm is an instance of what is called transfer learning in ma-transfer\\\\nlearning\\\\nchine learning: the method of acquiring knowledge from one task or domain, and\\\\nthen applying it (transferring it) to solve a new task.\\\\nThe second idea that we introduce in this chapter is the idea of contextual em-\\\\nbeddings : representations for words in context. The methods of Chapter 6 like\\\\nword2vec or GloVe learned a single vector embedding for each unique word win\\\\n\\\\nhow to use them well. And I\\\\'ve noticed this is something that really not many other classes teach. And this is something I\\\\'m rea lly convinced is a huge deal, and so by the \\\\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\\\nreally good at applying these learning algor ithms and getting them to work amazingly \\\\nwell in many problems. Okay? \\\\nLet\\\\'s see. So [inaudible] the board. After lear ning theory, there\\\\'s a nother class of learning \\\\nalgorithms that I then want to teach you a bout, and that\\\\'s unsupervised learning. So you\\\\n\\\\nwere called. He’s using it to explore the myster ious problem of how the brain learns. This \\\\nperceptron is being trained to recognize the difference between males and females. It is \\\\nsomething that all of us can do easily, but fe w of us can explain how. To get a computer \\\\nto do this it would involve working out ma ny complex rules about faces and writing a \\\\ncomputer program, but this perceptron was si mply given lots and lots of examples, \\\\nincluding some with unus ual hairstyles. But when it comes to a beetle the computer looks\\\\n\\\\norders? Or would th eta [inaudible]? \\\\nInstructor (Andrew Ng) :All great questions. The answer – so the question was, is this a \\\\ntypical hypothesis or can theta be a function of other variables and so on. And the answer \\\\nis sort of yes. For now, just for this first learning algorithm we\\\\'ll talk about using a linear \\\\nhypothesis class. A little bit ac tually later this quarter, we\\\\'ll talk about much more \\\\ncomplicated hypothesis classes, and we\\\\'ll actua lly talk about higher order functions as \\\\nwell, a little bit later today.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--c6acc751-d97b-4ca1-8d45-59642f62fe78-0', usage_metadata={'input_tokens': 702, 'output_tokens': 533, 'total_tokens': 1235, 'input_token_details': {'cache_read': 0}}),\n",
       "  AIMessage(content='Based on the context provided, the types of machine learning mentioned are:\\n\\n*   **Transfer learning**\\n*   **Unsupervised learning**\\n\\nThe context also describes a perceptron being trained with examples to recognize differences, which is characteristic of **supervised learning**, though the term itself is not explicitly used.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d1fdc398-c605-4e55-a894-acc7649d8b6a-0', usage_metadata={'input_tokens': 590, 'output_tokens': 480, 'total_tokens': 1070, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 418}}),\n",
       "  HumanMessage(content='Which ML type is actually better?', additional_kwargs={}, response_metadata={}, id='ffdf2b42-022a-498e-9fa0-9c608f6e56d1'),\n",
       "  HumanMessage(content='types of manchine learning', additional_kwargs={}, response_metadata={}, id='1807649e-0e50-41d2-9971-7b5548e6767e'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'retriever_tool', 'arguments': '{\"query\": \"Which ML type is actually better?\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--20551779-a00b-46e2-9342-2c3cbb0964b2-0', tool_calls=[{'name': 'retriever_tool', 'args': {'query': 'Which ML type is actually better?'}, 'id': '45c80fd7-f464-4229-8cdd-7536b7f07a02', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1313, 'output_tokens': 228, 'total_tokens': 1541, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 206}}),\n",
       "  ToolMessage(content='accurate on a training set. So given some feat ures, X, and some correct prices, Y, we \\nmight want to make that thet a square difference between th e prediction of the algorithm \\nand the actual price [inaudible].  \\nSo to choose parameters theta, unless we wa nt to minimize over the parameters theta, so \\nthe squared area between the pred icted price and the actual pri ce. And so going to fill this \\nin. We have M training examples. So the sum from I equals one through M of my M \\ntraining examples, of price predicted on the It h house in my training set. Mine is the\\n\\nout that it is often much easier to maximize the log of the likelihood rather than maximize \\nthe likelihood. So the log likelihood L of theta is just log of capital L. This will, therefore, \\nbe sum of this. Okay? And so to fit the para meters theta of our model we’ll find the value \\nof theta that maximizes this log likelihood. Yeah?  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Say that again.  Student: YI is [inaudible].  \\nInstructor (Andrew Ng) :Oh, yes. Thanks. So having maximized this function – well, it\\n\\nparameters, how do you estimate the parameters theta? So given a training set, what \\nparameters theta do you want to choose for your model? Well, the principle of maximum \\nlikelihood estimation says that, right? You can choose the valu e of theta that makes the \\ndata as probable as possible, right? So choose theta to maximize the likelihood. Or in \\nother words choose the parameters that make th e data as probable as possible, right? So \\nthis is massive likely your estimation from si x to six. So it’s choose the parameters that\\n\\nGaussian. So that no matter what sigma squared is since it’s a positive number the value of theta we end up with will be the same, right? So because minimizing this you get the \\nsame value of theta no matter what sigma squared is. So it’s as if in this model the value \\nof sigma squared doesn’t really  matter. Just remember that for the next lecture. We’ll \\ncome back to this again. Any questions a bout this? Actually, let me clean up another \\ncouple of boards and then I’ll see what questions you have.  \\nOkay. Any questions? Yeah?', name='retriever_tool', id='805ccaea-4ca1-4071-9f45-0724ba984331', tool_call_id='45c80fd7-f464-4229-8cdd-7536b7f07a02'),\n",
       "  AIMessage(content=\"I can't find it\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, name='RetriverAgent', id='run--17049dc3-5486-4d0c-938c-fff0deeef762-0', usage_metadata={'input_tokens': 1886, 'output_tokens': 135, 'total_tokens': 2021, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 129}}),\n",
       "  AIMessage(content=\"I apologize, but I'm not sure I fully understand your question. You mentioned 'types' and 'manchine learning'. Could you please rephrase your question? My apologies.\", additional_kwargs={}, response_metadata={}, id='62c01197-ff0c-43b3-acd9-d3f257deea21')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = graph.invoke( {\"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Which ML type is actually better?\",\n",
    "            }\n",
    "        ]} ,  config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
