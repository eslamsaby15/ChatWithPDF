{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f15d59",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A simple **Chat-with-Multi PDFs / Docs** app for **Retrieval-Augmented Generation (RAG)**.  \n",
    "It allows you to ask questions about the contents of PDFs and DOC files, and the app will provide relevant responses.\n",
    "\n",
    "![Chatbot Diagram](../src/images/GraghWorkFlow.jpg)\n",
    "\n",
    "This app is built using [**LangGraph**](https://www.langchain.com/langgraph) and [**LangChain**](https://www.langchain.com/).\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "## üöÄ Features covered\n",
    "\n",
    "- **Multi-Document Support**: Reads and processes both PDF and DOC files.  \n",
    "- **Text Chunking**: Splits extracted text into manageable overlapping chunks.  \n",
    "- **Semantic Embeddings**: Generates vector embeddings using state-of-the-art models.  \n",
    "- **Vector Database**: Stores embeddings in a scalable vector DB for fast similarity search.  \n",
    "- **Detection of Language and Dialect**: Automatically detects language and Arabic dialects if applicable.  \n",
    "- **Retriever Tool**: Fetches the most relevant chunks using semantic search.  \n",
    "- **Graders**: Evaluate the relevance of retrieved documents before generating answers.  \n",
    "- **Translate & Reasoning**: Provides translation support and reasoning over queries.  \n",
    "- **Query Handling**: Processes user queries for context-aware responses.  \n",
    "- **LLM-Powered Answering**: Combines retrieved context with queries to generate factual, context-aware answers.  \n",
    "- **Streamlit Integration**: Fully interactive interface for uploading documents and chatting with the AI assistant.\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Setting\n",
    "import os \n",
    "import dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ['GOOGLE_API_KEY'] = dotenv.get_key(key_to_get='GOOGLE_API_KEY', dotenv_path='.env')\n",
    "llm = init_chat_model(model = 'gemini-2.5-flash' , model_provider='google-genai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3005b",
   "metadata": {},
   "source": [
    "# A. Preprocess documents\n",
    "\n",
    "- In this step we read files to get the full text and split them into small chunks and embed them to store in Vectore DataBase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5748e75",
   "metadata": {},
   "source": [
    "##### 1) Reading Files using PyPDF2 to extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def get_content_pages(files:  list[str]) -> str:\n",
    "    \"\"\"Read PDF or TXT files and return their combined text\"\"\"\n",
    "    ftext = \"\"\n",
    "    for file_path in files:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            pdf_loader = PdfReader(file_path)\n",
    "            for page in pdf_loader.pages:\n",
    "                ftext += page.extract_text() or \"\"  \n",
    "        else:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                ftext += f.read()\n",
    "        ftext += \"\\n\"\n",
    "    return ftext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d16b1",
   "metadata": {},
   "source": [
    "##### 2) Split Full text into small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_chunks(text : str , chunk_size :int = 600, chunk_overlap : int = 50) :\n",
    "    \"\"\"Split to small and overla chunks\"\"\"\n",
    "    splitter= RecursiveCharacterTextSplitter(\"\\n\", chunk_size =chunk_size , \n",
    "                                         chunk_overlap =chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af101dd1",
   "metadata": {},
   "source": [
    "##### 3) Initialize Vector Database\n",
    "\n",
    "1. Generate embeddings from chunks using the **[MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)** model from SentenceTransformers.\n",
    "\n",
    "2. Store the embeddings in a vector database using **Chroma** from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "def create_vectorDB(chunks : list[str] ,\n",
    "                    embedding_name  : str = \"sentence-transformers/all-MiniLM-L6-v2\" ) : \n",
    "    \"\"\"Generate and store embedding of chunks into vector database\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name = embedding_name)\n",
    "    vectordb = InMemoryVectorStore.from_texts(chunks , embedding_model)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a14f2f",
   "metadata": {},
   "source": [
    "##### 4) concate components together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessDocuments(files:list[str]) :\n",
    "    \"\"\"concat all together\"\"\" \n",
    "    \n",
    "    content = get_content_pages(files=files)\n",
    "    chunks = split_chunks(content)\n",
    "    vectorDB = create_vectorDB(chunks)\n",
    "    return vectorDB , chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing functions\n",
    "pdfs = [os.path.join('data' , x) for x in os.listdir(\"data\")]  #pdf paths\n",
    "vector_db ,chunks = ProcessDocuments(pdfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is supervise learning?\" \n",
    "similar_docs = vector_db.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(\"---\" * 15, f\"*{i+1}*\", \"---\" * 15)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d2d2e",
   "metadata": {},
   "source": [
    "# B. Work-Flow-Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383e067",
   "metadata": {},
   "source": [
    "## 1) Define StateClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    detected_lang: str\n",
    "    dialect: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32a810",
   "metadata": {},
   "source": [
    "## 2) Generate a Retriever Tool using LangChain\n",
    "\n",
    "- Use `create_retriever_tool` from **LangChain** to generate a retriever tool from the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95436b8",
   "metadata": {},
   "source": [
    "##### 2.1) retriever_tool Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool \n",
    "\n",
    "retriver = vector_db.as_retriever()\n",
    "retrivertool = create_retriever_tool(retriver , \n",
    "                                     \"retriever_tool\" ,\n",
    "                                     \"Search and return information about input context\" )\n",
    "\n",
    "print(retrivertool.invoke({\"query\": query}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa9197",
   "metadata": {},
   "source": [
    "##### 2.2) retriever Agient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b881946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RetriverAgent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[retrivertool],\n",
    "    name=\"RetriverAgent\",\n",
    "    prompt=(\n",
    "        \"You are a retriever agent.\\n\"\n",
    "        \"- Otherwise, always call the retriever_tool with the user query.\\n\"\n",
    "        \"- Return ONLY the page_content of the retrieved documents.\\n\"\n",
    "        \"- Do not summarize or rephrase.\\n\"\n",
    "        \"- Don't return repeated retrieved chunks.\\n\"\n",
    "        \"- If you didn't find similar text return 'I can't find it'.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b8abf",
   "metadata": {},
   "source": [
    "## 3) Detect Language and Dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5a5e3",
   "metadata": {},
   "source": [
    "##### 3.1) Detect Language using Prompt Template with structure ouput and Dialect if the query language is ar\n",
    "\n",
    "#####  Detect Arabic dialect\n",
    "\n",
    "- Using [IbrahimAmin/marbertv2-arabic-written-dialect-classifier](https://huggingface.co/IbrahimAmin/marbertv2-arabic-written-dialect-classifier)  \n",
    "- The model predicts one of **5 Arabic dialects**:\n",
    "\n",
    "| Code | Dialect | Region / Notes |\n",
    "|------|---------|----------------|\n",
    "| MAGHREB | Maghreb dialect | Northwest Africa (Morocco, Algeria, Tunisia, Libya, Mauritania) |\n",
    "| LEV     | Levantine dialect | Lebanon, Syria, Jordan, Palestine |\n",
    "| MSA     | Modern Standard Arabic | Formal Arabic (books, news, official use) |\n",
    "| GLF     | Gulf dialect | Saudi Arabia, UAE, Kuwait, Bahrain, Qatar, Oman |\n",
    "| EGY     | Egyptian dialect | Egypt |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fafd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "class LanguageDetector(BaseModel):\n",
    "    language: str = Field(\n",
    "        description=\"Detected language of the question, represented in a two-character ISO 639-1 code.\"\n",
    "    )\n",
    "\n",
    "\n",
    "dialect_model_name = \"IbrahimAmin/marbertv2-arabic-written-dialect-classifier\"\n",
    "\n",
    "dialect_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(dialect_model_name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(dialect_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def detecting_language(state: MyState):\n",
    "    \n",
    "   \n",
    "    question = state['messages'][0].content\n",
    "    \n",
    "    detectionmodel = llm.with_structured_output(LanguageDetector)\n",
    "\n",
    "    LANGUAGE_DETECTOR_TEMPLATE = \"\\n\\n\".join([\n",
    "        \"You are a language detector assessing to return the language of the question from a user.\",\n",
    "        \"Here is the user question: {question}\",\n",
    "        \"# Instructions:\",\n",
    "        \"- Return only the two-character ISO 639-1 code for the language.\",\n",
    "        \"- Base detection on the language of the question itself (its structure and wording), not on individual foreign words inside it.\",\n",
    "        \"- Focus especially on the interrogative word (e.g., what, how, ŸÖŸÜ, ŸÖÿßÿ∞ÿß) and the main verb or auxiliary verb.\"\n",
    "    ])\n",
    "\n",
    "\n",
    "    detection_prompt = PromptTemplate(\n",
    "        template=LANGUAGE_DETECTOR_TEMPLATE,\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = detection_prompt.format(question=question)\n",
    "    response: LanguageDetector = detectionmodel.invoke(prompt)\n",
    "\n",
    "\n",
    "    # dialect\n",
    "    dialect = None\n",
    "    if response.language == \"ar\":\n",
    "        preds = dialect_pipeline(question, top_k=None)\n",
    "        if preds:\n",
    "            best = max(preds, key=lambda x: x[\"score\"])\n",
    "            dialect = best[\"label\"]\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"detected_lang\": response.language,\n",
    "        \"dialect\": dialect\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "\n",
    "examples = [\n",
    "    (\"Arabic Question\", MessagesState(messages=[HumanMessage(content=\"ŸÖÿß ŸáŸä ÿ£ŸÜŸàÿßÿπ Transformerÿü\")])),\n",
    "    (\"English Question\", MessagesState(messages=[HumanMessage(content=\"What are the types of Transformer?\")])),\n",
    "    (\"French Question\", MessagesState(messages=[HumanMessage(content=\"Qu'est-ce qu'un transformateur?\")])),\n",
    "    (\"Spanish Question\", MessagesState(messages=[HumanMessage(content=\"¬øCu√°les son los tipos de transformadores?\")])),\n",
    "    (\"German Question\", MessagesState(messages=[HumanMessage(content=\"Welche Arten von Transformatoren gibt es?\")])) ,\n",
    "]\n",
    "\n",
    "\n",
    "for name, state in examples:\n",
    "    result = detecting_language(state)\n",
    "    print(f\"{name}: {result['detected_lang'] ,result['dialect'] }\")\n",
    "    print(\"-\" * 50)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "import time\n",
    "\n",
    "examples = [\n",
    "    (\"EGY Example\", MessagesState(messages=[HumanMessage(content=\"ÿßÿ≤ŸäŸÉ Ÿäÿµÿ≠ÿ®Ÿä ŸÇŸàŸÑŸä ÿßŸÜŸàÿßÿπ l\")])),\n",
    "    (\"LEV Example 2\", MessagesState(messages=[HumanMessage(content=\"ÿπÿßŸÖŸÑ ÿßŸä Ÿäÿßÿ≤ŸÑŸÖŸä ÿßŸÑŸäŸàŸÖ ŸÖŸÖŸÉŸÜ ÿ™ŸÇŸàŸÑŸä types of ML\")])),\n",
    "    (\"GLF Example\", MessagesState(messages=[HumanMessage(content=\"ÿ¥ŸÑŸàŸÜŸÉ Ÿäÿß ÿ∑ŸàŸäŸÑ ÿßŸÑÿπŸÖÿ±ÿü ŸÖŸÖŸÉŸÜ ÿ™ŸÇŸàŸÑ ŸÑŸä examples of ML\")])),\n",
    "    (\"LEV Example\", MessagesState(messages=[HumanMessage(content=\"ŸÉŸäŸÅŸÉ Ÿäÿß ÿ≤ŸÑŸÖŸäÿü ÿ¥Ÿà ÿßŸÑÿ£ÿÆÿ®ÿßÿ±ÿü give me types of ML\")])),\n",
    "    (\"MSA Example\", MessagesState(messages=[HumanMessage(content=\"ŸÖÿß ŸáŸä ÿ£ŸÜŸàÿßÿπ   ML techniquesÿü\")]))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for name, state in examples:\n",
    "    result = detecting_language(state)\n",
    "    print(f\"{name}: {result['detected_lang'] ,result['dialect'] }\")\n",
    "    print(\"-\" * 50)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f172ea",
   "metadata": {},
   "source": [
    "## 4) Translate Query to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d73a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "def TranslateQuery(state :MyState):\n",
    "    \n",
    "    \"Machine Translation to translate Queries to english text\"\n",
    "    \n",
    "    user_messages = [m for m in state[\"messages\"] if isinstance(m,HumanMessage)]\n",
    "    question = state.get(\"translated_query\", user_messages[-1].content)\n",
    "\n",
    "    msg_prompt = f\"\"\"\n",
    "                    You are a Machine Translation (MT) system.\n",
    "                    Your task: translate the user question to English text.\n",
    "\n",
    "                    Instructions:\n",
    "                    1. Translate the question to English as accurately as possible.\n",
    "                    2. Do not add explanations, comments, or extra content.\n",
    "                    3. Do not attempt to clarify or modify the meaning.\n",
    "                    4. Keep the original meaning exactly.\n",
    "\n",
    "                    User question: \"{question}\"\n",
    "                    \"\"\"\n",
    "\n",
    "    prompt_msg_template = PromptTemplate(\n",
    "        template=msg_prompt,\n",
    "        input_variables=['question']\n",
    "    )\n",
    "\n",
    "    resonong = llm.invoke([{'role': 'user', 'content': prompt_msg_template.format(question=question)}])\n",
    "\n",
    "    return {'messages': [HumanMessage(content=resonong.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input_state = {\n",
    "    \"messages\": convert_to_messages([\n",
    "        {\"role\": \"user\", \"content\": \"ÿ™ŸÇÿØÿ± ÿ™ŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÜŸàÿßÿπ transformers \"}\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "response = TranslateQuery(input_state)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be0803",
   "metadata": {},
   "source": [
    "## 5) Grader\n",
    "- Grander is a score computed from llm to determine whether the retrieved documents are relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e612d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "class GraderDocument(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "    binary_score : str = Field(description= \"Relevance score: 'yes' if relevant, or 'no' if not relevant\")\n",
    "\n",
    "\n",
    "def GraderDocumentAgent(state: MyState)-> Literal['Chit-ChatAgent' ,'AnswerAgent'] :\n",
    "    \n",
    "    user_messages = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)]\n",
    "    question = state.get(\"translated_query\", user_messages[-1].content)\n",
    "    context = state['messages'][-1].content\n",
    "\n",
    "      \n",
    "    gradermodel = llm.with_structured_output(GraderDocument)\n",
    "    \n",
    "    GRADE_PROMPT = \"\\n\\n\".join([\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\",\n",
    "    \"Here is the retrieved document: \\n\\n {context}\",\n",
    "    \"Here is the user question: {question}\",\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\",\n",
    "    \"If the question is an introductory or personal question (e.g., greetings like 'how are you', or self-introduction like 'I am X'), always grade it as 'yes'.\",\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\" ])\n",
    "    \n",
    "    prompt = PromptTemplate(template= GRADE_PROMPT , input_variables=['question', 'context'])\n",
    "    \n",
    "    \n",
    "    prompt_template = prompt.format(question = question , context = context)\n",
    "    response =  gradermodel.invoke(\n",
    "        [HumanMessage(content=prompt_template) ]\n",
    "    )\n",
    "\n",
    "    score = response.binary_score\n",
    "    \n",
    "    if score == 'yes' :\n",
    "        return \"AnswerAgent\"\n",
    "    else : \n",
    "        return 'Chit-ChatAgent'\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82013ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does machine learning?\",\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retriever_tool\",\n",
    "                        \"args\": {\"query\": \"Supervised learning, Unsupervised learning, Reinforcement learning.\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "GraderDocumentAgent(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69180bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does machine learning?\",\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retriever_tool\",\n",
    "                        \"args\": {\"query\": \"Supervised learning, Unsupervised learning, Reinforcement learning.\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \n",
    "             \"content\": \"\"\"Machine learning is a field of artificial intelligence that focuses on building models that can learn from data and make predictions or decisions without being explicitly programmed. \n",
    "                            It is commonly used for tasks likelassification, regression, and pattern recognition. \"\"\", \n",
    "            \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "GraderDocumentAgent(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098f4f2",
   "metadata": {},
   "source": [
    "## 6) Generate answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateAnswer(state: MyState) :\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "\n",
    "    GENERATE_PROMPT = \"\\n\".join([\n",
    "        \"You are an assistant for question-answering tasks.\",\n",
    "        \"Use the following pieces of retrieved context to answer the question.\",\n",
    "        \"- you must first understand the question and the context to answer correctly.\",\n",
    "        \"Answer as many questions as possible and make it a simple temporary one.\",\n",
    "        \"- Generate english text only.\",\n",
    "        \"- If the question is a greeting or introductory (like 'how are you', 'what's up', 'hello', 'hi', or self-introduction like 'I am X'), do not use the context. Instead, just greet back politely and say 'How can I help you?'.\",\n",
    "        \"- If it is a normal question, add some information from the context in your answer to make it complete, not only from the context.\",\n",
    "        \"Question: {question}\\n\",\n",
    "        \"Context: {context}\"\n",
    "    ])\n",
    "\n",
    "    user_messages = [m for m in state[\"messages\"] if isinstance(m,HumanMessage)]\n",
    "    \n",
    "    question = state.get(\"translated_query\", user_messages[-1].content)\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm.invoke([{\"role\": \"user\", \n",
    "                            \"content\": prompt}])\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the main types of machine learning?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_tool\",\n",
    "                        \"args\": {\"query\": \"types of machine learning\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Machine learning is commonly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = GenerateAnswer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1cd65",
   "metadata": {},
   "source": [
    "## 7) Chit-Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f00c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def chitChatAgent(state : MyState):\n",
    "    user_messages= [m for m in state['messages'] if isinstance(m,HumanMessage)]\n",
    "    question = state.get(\"translated_query\", user_messages[-1].content)\n",
    "\n",
    "    msg_prompt = f\"\"\"\n",
    "        You are a Chit-Chat Assistant.\n",
    "        Your task: reply politely when the context not related to  a user question so follow Instructions to senf a chit chat message.\n",
    "        \n",
    "        Instructions:\n",
    "        1. Start by apologizing that you don't fully understand the question.\n",
    "        2. Try to clarify by highlighting key words from the user's question.\n",
    "        3. Use short, simple sentences to suggest that the user rephrase their question.\n",
    "        5. shorts apologizing messages in first sentence only.\n",
    "        6. write in english text only.\n",
    "        7. don't require any language from user to write his question.\n",
    "        8. ask only some question trying to understand user question\n",
    "        \n",
    "\n",
    "        User question: \"{question}\"\n",
    "        \"\"\"\n",
    "\n",
    "    prompt_msg_template = PromptTemplate(\n",
    "        template=msg_prompt,\n",
    "        input_variables=['question']\n",
    "    )\n",
    "\n",
    "    resonong = llm.invoke([{'role': 'user', 'content': prompt_msg_template.format(question=question)}])\n",
    "\n",
    "    return {'messages': [AIMessage(content=resonong.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790dde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"where salaj play in zamilk\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_tool\",\n",
    "                        \"args\": {\"query\": \"classification of machine learning\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Supervised, Unsupervised, Reinforcement Learning\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = chitChatAgent(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"ÿßŸäŸÜ ŸÉŸÜÿ™ ŸàŸäŸÜ ŸäÿßŸàŸÑÿß\",\n",
    "            }\n",
    "           \n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = chitChatAgent(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e3fa4",
   "metadata": {},
   "source": [
    "## 8)  Translate Agent\n",
    "\n",
    "- Translate the reasoning into the same **language** and **dialect** of the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslationReasoning(state: MyState):\n",
    "    \"\"\"Translate the reasoning into the same **language** and **dialect** of the question.\"\"\"\n",
    "\n",
    "    context = state[\"messages\"][-1].content\n",
    "    detected_lang = state.get(\"detected_lang\")\n",
    "    dialect = state.get(\"dialect\")\n",
    "\n",
    "\n",
    "    Translatetemplate = \"\\n\".join([\n",
    "        \"You are a translation agent. Your ONLY job is to translate English text into the target language below.\",\n",
    "        \"Never answer in Spanish, French, or any other language unless it exactly matches the detected language.\",\n",
    "        \"you must know we shortcut the two-character ISO 639-1 code for the language like ar for arabic , en for english \",\n",
    "        \"\",\n",
    "        f\"Target language: {detected_lang} \",\n",
    "        f\"Target dialect: {dialect or 'standard'}\",\n",
    "        \"\",\n",
    "        \"# Instructions:\",\n",
    "        \"- If dialect is None, translate to the language only.\",\n",
    "        \"- If the language is not English, keep important keywords in English.\",\n",
    "        \"- Don't explain, don't rephrase, just translate.\",\n",
    "        \"- If target language is Arabic, mimic the dialect if possible; otherwise use Modern Standard Arabic.\",\n",
    "        \"\",\n",
    "        \"Text to translate to targey language : \",\n",
    "        \"{context}\"\n",
    "    ])\n",
    "\n",
    "    TranslatePrompt = PromptTemplate(\n",
    "        template=Translatetemplate,\n",
    "        input_variables=[\"context\"],\n",
    "    )\n",
    "\n",
    "    prompt = TranslatePrompt.format(context=context)\n",
    "\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"You are a strict translation agent. Respond ONLY with the translated text.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    translated_text = response.content \n",
    "    return {\"messages\": [AIMessage(content=translated_text)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello Islam! I'm doing well, thank you for asking! How can I help you?\")  \n",
    "    ],\n",
    "    \"detected_lang\": \"ar\",    \n",
    "    \"dialect\": \"EGY\"        \n",
    "}\n",
    "\n",
    "result = TranslationReasoning(test_state)\n",
    "\n",
    "print(\"\\n================ Translation Output ================\\n\")\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f629be",
   "metadata": {},
   "source": [
    "# Work Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ba718",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Language & Dialect Detection üåê\n",
    "- Detect the language of the user query.\n",
    "- Detect dialects if the query is in Arabic or other supported languages.\n",
    "- Translate the query to English for processing.\n",
    "\n",
    "### 2. Database Preparation üóÇÔ∏è\n",
    "- Extract text from PDF/DOC files.\n",
    "- Split text into smaller overlapping chunks.\n",
    "- Generate embeddings and store them in the vector database.\n",
    "\n",
    "### 3. Retrieval & Grading üîé‚öôÔ∏è\n",
    "- Use the retriever tool to fetch relevant chunks based on the translated query.\n",
    "- Graders evaluate the relevance of the retrieved chunks.\n",
    "- Decide workflow:\n",
    "  - **Generate Answer** ‚Üí if relevant chunks are found.\n",
    "  - **Chitchat / fallback** ‚Üí if no relevant context is retrieved.\n",
    "\n",
    "### 4. Answer Generation ‚ú®\n",
    "- Combine retrieved context (if any) with the translated query.\n",
    "- Pass to the LLM ‚Üí produce answer in English.\n",
    "\n",
    "### 5. Output Translation & Presentation üåç\n",
    "- Translate the generated answer back to the original language and dialect of the query.\n",
    "- Display the answer in Streamlit with proper formatting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97f1bb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "workflow = StateGraph(MyState)\n",
    "\n",
    "workflow.add_node(\"DetectLangAgent\", detecting_language)\n",
    "workflow.add_node(\"Translate_Query\", TranslateQuery)\n",
    "workflow.add_node(\"RetriverAgent\", RetriverAgent)\n",
    "workflow.add_node(\"Chit-ChatAgent\", chitChatAgent)\n",
    "workflow.add_node(\"AnswerAgent\", GenerateAnswer)\n",
    "workflow.add_node(\"TranslationReasoning\", TranslationReasoning)\n",
    "\n",
    "workflow.add_edge(START, \"DetectLangAgent\")\n",
    "workflow.add_edge(\"DetectLangAgent\", \"Translate_Query\")\n",
    "workflow.add_edge(\"Translate_Query\", \"RetriverAgent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"RetriverAgent\",\n",
    "    GraderDocumentAgent,\n",
    "    {\n",
    "        \"AnswerAgent\": \"AnswerAgent\",\n",
    "        \"Chit-ChatAgent\": \"Chit-ChatAgent\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"AnswerAgent\", \"TranslationReasoning\")\n",
    "workflow.add_edge(\"Chit-ChatAgent\", \"TranslationReasoning\")\n",
    "workflow.add_edge(\"TranslationReasoning\", END)\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "user_message = HumanMessage(content=\"ÿßÿ∞ŸäŸÉ Ÿäÿßÿµÿ≠ÿ®Ÿä ÿßŸÜÿß ÿßÿ≥ŸÑÿßŸÖ ÿπÿßŸÖŸÑ ÿßŸä\")\n",
    "\n",
    "stream = graph.stream(\n",
    "    {\"messages\": [user_message]},\n",
    "    config ={\"configurable\": {\"thread_id\": \"6\"}},\n",
    "    stream_mode=\"values\"  \n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if \"messages\" in chunk:\n",
    "        for msg in chunk[\"messages\"]:\n",
    "            msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke( {\"messages\": [\n",
    "            HumanMessage(content=\" ŸäÿπŸÜŸä ÿßŸä decoding\") \n",
    "        ]} ,  config={\"configurable\": {\"thread_id\": \"6\"}})\n",
    "\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edcbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke( {\"messages\": [\n",
    "            HumanMessage(content=\"what is encoder decoder\") \n",
    "        ]} ,  config={\"configurable\": {\"thread_id\": \"6\"}})\n",
    "\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038fef3",
   "metadata": {},
   "source": [
    "## üîπ Conclusion  \n",
    "\n",
    "- Demonstrates how **Chat with Multiple PDFs / Docs** pipeline can be built using **RAG (Retrieval-Augmented Generation)**.  \n",
    "\n",
    "- Steps:  \n",
    "\n",
    "  1. **Language & Dialect Detection** ‚Üí detect the user query language and dialect, translate to English for processing.  \n",
    "\n",
    "  2. **Database Preparation** ‚Üí extract text from PDF/DOC files, split text into overlapping chunks, create embeddings, and store them in the vector database.  \n",
    "\n",
    "  3. **Retrieval & Grading** ‚Üí fetch relevant chunks using the retriever tool, grade their relevance, and decide workflow:\n",
    "     - **Generate Answer** ‚Üí if relevant chunks are found.\n",
    "     - **Chitchat / fallback** ‚Üí if no relevant context is retrieved.  \n",
    "\n",
    "  4. **Answer Generation** ‚Üí combine retrieved context with the translated query, generate the answer via LLM.  \n",
    "\n",
    "  5. **Output Translation & Presentation** ‚Üí translate the generated answer back to the original language and dialect, display it in Streamlit with proper formatting.  \n",
    "---------\n",
    "\n",
    "## üë®‚Äçüíª Built by Eslam Sabry\n",
    "\n",
    "concat\n",
    "üîó [LinkedIn](https://www.linkedin.com/in/eslamsabryai)   üîó [Kaggle](https://www.kaggle.com/eslamsabryelsisi)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadb85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
